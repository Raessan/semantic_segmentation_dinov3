{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43650d53-7aba-43c9-898f-60a7726481c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_coco import DatasetCOCOPanoptic\n",
    "from src.model_backbone import DinoBackbone\n",
    "from src.model_head import ASPPDecoder\n",
    "from src.loss_focal_dice import SemanticLoss\n",
    "import config.config as cfg\n",
    "from src.common import tensor_to_image\n",
    "from src.utils import visualize_maps, outputs_to_maps\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63c50f-e5c1-4798-bfe6-a4333f0ff37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "################ LOAD ALL THE PARAMETERS #############################\n",
    "# DATASET PARAMETERS\n",
    "COCO_ROOT = cfg.COCO_ROOT\n",
    "IMG_SIZE = cfg.IMG_SIZE\n",
    "PATCH_SIZE = cfg.PATCH_SIZE\n",
    "PROB_AUGMENT_TRAINING = cfg.PROB_AUGMENT_TRAINING\n",
    "PROB_AUGMENT_VALID = cfg.PROB_AUGMENT_VALID\n",
    "IMG_MEAN = cfg.IMG_MEAN\n",
    "IMG_STD = cfg.IMG_STD\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "DINOV3_DIR = cfg.DINOV3_DIR\n",
    "DINO_MODEL = cfg.DINO_MODEL\n",
    "DINO_WEIGHTS = cfg.DINO_WEIGHTS\n",
    "MODEL_TO_NUM_LAYERS = cfg.MODEL_TO_NUM_LAYERS\n",
    "MODEL_TO_EMBED_DIM = cfg.MODEL_TO_EMBED_DIM\n",
    "HIDDEN_DIM = cfg.HIDDEN_DIM\n",
    "TARGET_SIZE = cfg.TARGET_SIZE\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "BATCH_SIZE = cfg.BATCH_SIZE\n",
    "WEIGHT_LOSS_DICE = cfg.WEIGHT_LOSS_DICE\n",
    "WEIGHT_LOSS_FOCAL = cfg.WEIGHT_LOSS_FOCAL\n",
    "\n",
    "LEARNING_RATE = cfg.LEARNING_RATE\n",
    "WEIGHT_DECAY = cfg.WEIGHT_DECAY\n",
    "NUM_EPOCHS = cfg.NUM_EPOCHS\n",
    "NUM_SAMPLES_PLOT = cfg.NUM_SAMPLES_PLOT\n",
    "\n",
    "LOAD_MODEL = cfg.LOAD_MODEL\n",
    "SAVE_MODEL = cfg.SAVE_MODEL\n",
    "MODEL_PATH_TRAIN_LOAD = cfg.MODEL_PATH_TRAIN_LOAD\n",
    "RESULTS_PATH = cfg.RESULTS_PATH\n",
    "\n",
    "train_set = DatasetCOCOPanoptic(COCO_ROOT, \"train\", IMG_SIZE, PATCH_SIZE, PROB_AUGMENT_TRAINING, IMG_MEAN, IMG_STD)\n",
    "train_dataloader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers=8, shuffle=True)\n",
    "\n",
    "val_set = DatasetCOCOPanoptic(COCO_ROOT, \"val\", IMG_SIZE, PATCH_SIZE, PROB_AUGMENT_VALID, IMG_MEAN, IMG_STD)\n",
    "val_dataloader = DataLoader(val_set, batch_size = BATCH_SIZE, num_workers=8, shuffle=True)\n",
    "\n",
    "num_classes = len(train_set.class_names)\n",
    "\n",
    "dino_model = torch.hub.load(\n",
    "        repo_or_dir=DINOV3_DIR,\n",
    "        model=DINO_MODEL,\n",
    "        source=\"local\",\n",
    "        weights=DINO_WEIGHTS\n",
    ")\n",
    "n_layers_dino = MODEL_TO_NUM_LAYERS[DINO_MODEL]\n",
    "embed_dim = MODEL_TO_EMBED_DIM[DINO_MODEL]\n",
    "\n",
    "dino_backbone = DinoBackbone(dino_model, n_layers_dino).to(device)\n",
    "\n",
    "model_head = ASPPDecoder(num_classes=len(train_set.class_names), in_ch=embed_dim,\n",
    "                                target_size=(TARGET_SIZE, TARGET_SIZE)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_head.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Load model\n",
    "if LOAD_MODEL:\n",
    "    model_head.load_state_dict(torch.load(MODEL_PATH_TRAIN_LOAD))\n",
    "    print(\"Model successfully loaded!\")\n",
    "\n",
    "# Freeze parameters\n",
    "for p in dino_backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "n_params = sum([p.numel() for p in dino_backbone.parameters()]) + sum([p.numel() for p in model_head.parameters()])\n",
    "print(\"Total number of parameters: \", n_params)\n",
    "n_trainable_params = sum([p.numel() for p in dino_backbone.parameters() if p.requires_grad]) + sum([p.numel() for p in model_head.parameters() if p.requires_grad])\n",
    "print(\"Total number of trainable parameters: \", n_trainable_params)\n",
    "n_params_backbone = sum([p.numel() for p in dino_backbone.parameters()])\n",
    "print(\"Number parameters backbone: \", n_params_backbone)\n",
    "\n",
    "# Prepare loss fcn\n",
    "# Focal + dice\n",
    "loss_module = SemanticLoss(dice_weight=WEIGHT_LOSS_DICE, focal_weight=WEIGHT_LOSS_FOCAL, target_size=(TARGET_SIZE, TARGET_SIZE), ignore_index=None)\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    folder_path = f\"{RESULTS_PATH}/{current_date}\"\n",
    "    \n",
    "    json_params = { \n",
    "        \"IMG_SIZE\" : IMG_SIZE, \n",
    "        \"PATCH_SIZE\" : PATCH_SIZE, \n",
    "        \"PROB_AUGMENT_TRAINING\": PROB_AUGMENT_TRAINING,\n",
    "        \"PROB_AUGMENT_VALID\": PROB_AUGMENT_VALID,\n",
    "        \"DINO_MODEL\": DINO_MODEL,\n",
    "        \"HIDDEN_DIM\": HIDDEN_DIM,\n",
    "        \"TARGET_SIZE\": TARGET_SIZE,\n",
    "        \"WEIGHT_LOSS_DICE\": WEIGHT_LOSS_DICE,\n",
    "        \"WEIGHT_LOSS_FOCAL\": WEIGHT_LOSS_FOCAL,\n",
    "        \"LEARNING_RATE\" : LEARNING_RATE,\n",
    "        \"LOAD_MODEL\" : LOAD_MODEL,\n",
    "        \"MODEL_PATH_TRAIN_LOAD\" : MODEL_PATH_TRAIN_LOAD,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef2a77-dbda-4ee5-86ee-d9bdaff26d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINO backbone always in eval mode\n",
    "dino_backbone.eval()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##################### TRAIN #######################\n",
    "    model_head.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, semantic_targets, _) in enumerate(tqdm(train_dataloader)):\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        semantic_targets = semantic_targets.to(device, dtype=torch.long)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        feat = dino_backbone(images)\n",
    "        semantic_logits = model_head(feat)\n",
    "        \n",
    "        # Calculate loss\n",
    "        losses = loss_module(semantic_logits, semantic_targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss = losses[0]\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping and Optimize\n",
    "        # clip all gradients to max norm 5.0\n",
    "        torch.nn.utils.clip_grad_norm_(model_head.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += total_loss.item()\n",
    "\n",
    "        # Show loss\n",
    "        if (batch_idx % 400 == 0 and batch_idx > 0):\n",
    "            print(f\"Epoch {epoch+1}, batch {batch_idx}, Loss: {train_loss/(batch_idx+1)}, focal loss: {losses[1]}, dice loss: {losses[2]}\")\n",
    "\n",
    "        # Plot samples\n",
    "        if (batch_idx % 4000 == 0 and batch_idx > 0):\n",
    "            with torch.no_grad():\n",
    "                for i in range(min(NUM_SAMPLES_PLOT, BATCH_SIZE)):\n",
    "                    visualize_maps(tensor_to_image(images[i], IMG_MEAN, IMG_STD), semantic_targets[i].detach().cpu().numpy(), \n",
    "                                   class_names=train_set.class_names,\n",
    "                                   alpha=1.0,\n",
    "                                   figsize=(12, 8),\n",
    "                                   draw_semantic_labels=True,\n",
    "                                   semantic_label_fontsize=5,\n",
    "                                   seed=42)\n",
    "                    semantic_mask_pred = outputs_to_maps(semantic_logits[i],\n",
    "                                                        images.shape[2:],\n",
    "                                                        )\n",
    "                    visualize_maps(tensor_to_image(images[i], IMG_MEAN, IMG_STD), semantic_mask_pred, \n",
    "                                   class_names=train_set.class_names,\n",
    "                                   alpha=1.0,\n",
    "                                   figsize=(12, 8),\n",
    "                                   draw_semantic_labels=True,\n",
    "                                   semantic_label_fontsize=5,\n",
    "                                   seed=42)        \n",
    "\n",
    "    train_loss /= float(batch_idx+1)\n",
    "\n",
    "    ##################### VALIDATION #######################\n",
    "    model_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loss_dice = 0.0\n",
    "    val_loss_focal = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, semantic_targets, _) in enumerate(tqdm(val_dataloader)):\n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            semantic_targets = semantic_targets.to(device, dtype=torch.long)\n",
    "            \n",
    "            # Forward pass\n",
    "            feat = dino_backbone(images)\n",
    "            semantic_logits = model_head(feat)\n",
    "            \n",
    "            # Calculate loss\n",
    "            losses = loss_module(semantic_logits, semantic_targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss = losses[0]\n",
    "    \n",
    "            val_loss += total_loss.item()\n",
    "            val_loss_focal += losses[1].item()\n",
    "            val_loss_dice += losses[2].item()\n",
    "\n",
    "            # Plot samples\n",
    "            if batch_idx == 0:\n",
    "                for i in range(min(NUM_SAMPLES_PLOT, BATCH_SIZE)):\n",
    "                    visualize_maps(tensor_to_image(images[i], IMG_MEAN, IMG_STD), semantic_targets[i].detach().cpu().numpy(), \n",
    "                                   class_names=train_set.class_names,\n",
    "                                   alpha=1.0,\n",
    "                                   figsize=(12, 8),\n",
    "                                   draw_semantic_labels=True,\n",
    "                                   semantic_label_fontsize=5,\n",
    "                                   seed=42)\n",
    "                    semantic_mask_pred = outputs_to_maps(semantic_logits[i],\n",
    "                                                        images.shape[2:],\n",
    "                                                        )\n",
    "                    visualize_maps(tensor_to_image(images[i], IMG_MEAN, IMG_STD), semantic_mask_pred, \n",
    "                                   class_names=train_set.class_names,\n",
    "                                   alpha=1.0,\n",
    "                                   figsize=(12, 8),\n",
    "                                   draw_semantic_labels=True,\n",
    "                                   semantic_label_fontsize=5,\n",
    "                                   seed=42)  \n",
    "\n",
    "\n",
    "        val_loss /= float(batch_idx+1)\n",
    "        val_loss_focal /= float(batch_idx+1)\n",
    "        val_loss_dice /= float(batch_idx+1)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss}, val loss NN total: {val_loss}, val loss focal: {val_loss_focal},  val loss dice: {val_loss_dice}\")\n",
    "    \n",
    "    if SAVE_MODEL:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        # Save model and params\n",
    "        json_params_epoch = json_params.copy()\n",
    "        json_params_epoch[\"epoch\"] = epoch\n",
    "        json_params_epoch[\"train_loss\"] = train_loss\n",
    "        json_params_epoch[\"val_loss\"] = val_loss\n",
    "        json_params_epoch[\"val_loss_focal\"] = val_loss_focal\n",
    "        json_params_epoch[\"val_loss_dice\"] = val_loss_dice\n",
    "        model_path = os.path.join(folder_path,f\"model_{epoch}.pth\")\n",
    "        json_path = os.path.join(folder_path,f\"params_{epoch}.json\")\n",
    "        torch.save(model_head.state_dict(), model_path)\n",
    "        with open(json_path, \"w\") as outfile:\n",
    "            json.dump(json_params_epoch, outfile)\n",
    "    \n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a153aa-c94c-415b-9da6-a7f10b2025d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
